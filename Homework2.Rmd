---
title: "Homework 2"
author: "Thome, Julia"
date: "`r date()`"
output: md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library('ElemStatLearn')  ## for 'prostate'
library('splines')        ## for 'bs'
library('magrittr')       ## for '%<>%' operator
library('glmnet')         ## for 'glmnet'
library('kableExtra')     ## for 'kable'
library('dplyr')          ## for 'select', 'filter', and others
```

Use the prostate cancer data from the ElemStatLearn package for R.
```{r}
data('prostate')

# subset to training data
prostate_train <- prostate %>%
  filter(train ==TRUE) %>% 
  dplyr::select(-train)

#subset to testing data
prostate_test <- prostate %>%
  filter(train == FALSE) %>% 
   dplyr::select(-train)
```


Use the cor function to reproduce the correlations listed in HTF Table 3.1, page 50.
```{r}


# only using training data
pros_cor <- cor(prostate_train, method = "pearson")
pros_cor <- round(pros_cor[-c(1,9),-c(8,9)],3)
pros_cor[upper.tri(pros_cor)] <- NA
pros_cor
```

Treat lpsa as the oucome, and use all other variables in the data set as predictors.With the training subset, train a least-squares regression model with all predictors using the lm function (with the training subset).
```{r}
fit <- lm(lpsa ~ ., data = prostate_train)
summary(fit)
```


Use the testing subset to compute the test error using the fitted least-squares regression model.
```{r}
# L_2 loss function
L2_loss <- function(y, yhat)
  (y-yhat)^2

# error function
error <- function(dat, fit, loss=L2_loss)
  mean(loss(dat$lpsa, predict(fit, newdata=dat)))

# testing error
(test_error <- error(prostate_test, fit) %>% round(3))
```


Train a ridge regression model using the glmnet function, and tune the value of lambda.
```{r}
# setting up ridge regression model formula and input matrix
form  <- lpsa ~ 0 +  lcavol + lweight + age + lbph + lcp + pgg45  + svi + gleason
x_inp <- model.matrix(form, data=prostate_train) 
y_out <- prostate_train$lpsa

#
lambda = seq(2, 0, -0.05)
fit_rr <- glmnet(x=x_inp, y=y_out, alpha = 0, lambda=lambda) 
print(fit_rr$beta)

# Ridge regression error function
error <- function(dat, fit, lam, form, loss=L2_loss) {
  x_inp <- model.matrix(form, data=dat)
  y_out <- dat$lpsa
  y_hat <- predict(fit, newx=x_inp, s=lam)  ## see predict.elnet
  mean(loss(y_out, y_hat))
}

# testing error
err_test_1 <- sapply(fit_rr$lambda, function(lam) 
  error(prostate_test, fit_rr, lam, form))

# value of lambda to be chosen since it minimizes testing error
(lambda_tuned <- lambda[which.min(err_test_1)])
```


Create a path diagram of the ridge regression analysis
```{r}
# setting up plot
plot(x=range(fit_rr$lambda),
     y=range(as.matrix(fit_rr$beta)),
     type='n',
     xlab=expression(lambda),
     ylab='Coefficients',
     main = "Path Diagrams for Ridge Regression")

# plotting path for each coefficient for every lambda
for(i in 1:nrow(fit_rr$beta)) {
  points(x=fit_rr$lambda, y=fit_rr$beta[i,], pch=19, col='#00000055')
  lines(x=fit_rr$lambda, y=fit_rr$beta[i,], col='#00000055')
}
abline(h=0, lty=3, lwd=2)
```


Create a figure that shows the training and test error associated with ridge regression as a function of lambda
```{r}
# training error
err_train_1 <- sapply(fit_rr$lambda, function(lam) 
  error(prostate_train, fit_rr, lam, form))

# plotting testing error and training error
plot(x=range(fit_rr$lambda),
     y=range(c(err_train_1, err_test_1)),
     type='n',
     xlab=expression(lambda),
     ylab='train/test error')
points(fit_rr$lambda, err_train_1, pch=19, type='b', col='darkblue')
points(fit_rr$lambda, err_test_1, pch=19, type='b', col='darkred')
legend('topleft', c('train','test'), lty=1, pch=19,
       col=c('darkblue','darkred'), bty='n')

colnames(fit_rr$beta) <- paste('lam =', fit_rr$lambda)
print(fit_rr$beta %>% as.matrix)
```
